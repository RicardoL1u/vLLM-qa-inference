# vLLM-qa-inference
This repo contains the code for the inference of LLMs on the QA task with In-Context Learning. (based on vLLM)
